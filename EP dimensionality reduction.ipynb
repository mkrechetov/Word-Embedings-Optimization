{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import scipy\n",
    "\n",
    "from numpy.linalg import norm\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "\n",
    "def cos_dis(u,v):\n",
    "    dist = 1.0 - np.dot(u, v) / (norm(u) * norm(v))\n",
    "    return dist\n",
    "                \n",
    "def analogy(question_got, emb): \n",
    "    \n",
    "    question = sentence_to_wordlist(question_got)\n",
    "    country1_emb = emb[list_of_words.index(question[0]), :]\n",
    "    capital1_emb = emb[list_of_words.index(question[1]), :]\n",
    "    country2_emb = emb[list_of_words.index(question[2]), :]\n",
    "    target_emb = country2_emb - country1_emb + capital1_emb\n",
    "    \n",
    "    closest = -1\n",
    "    closest_value = float(\"inf\")\n",
    "    \n",
    "    for curr_word in list_of_words:\n",
    "        if (curr_word not in question[:3]):\n",
    "            if (cos_dis(emb[list_of_words.index(curr_word),:],target_emb) < closest_value):\n",
    "                closest_value = cos_dis(emb[list_of_words.index(curr_word),:],target_emb)\n",
    "                closest = list_of_words.index(curr_word)\n",
    "            \n",
    "    return (list_of_words[closest] == question[3])\n",
    "\n",
    "\n",
    "def test_emb(questions, embedding):\n",
    "    \n",
    "    CPUs = mp.cpu_count()\n",
    "    p = mp.Pool(CPUs)\n",
    "    \n",
    "    number_of_questions = len(list_of_questions)\n",
    "        \n",
    "    #compute in parallel\n",
    "    analogy_emb=partial(analogy, emb=embedding)\n",
    "    results = p.map(analogy_emb, questions)   \n",
    "\n",
    "    return sum(results)/number_of_questions\n",
    "\n",
    "def save_emb(V):\n",
    "    import json\n",
    "    dict = {}\n",
    "    for i in range(number_of_words):\n",
    "        dict[list_of_words[i]] = V[i, :].tolist()\n",
    "    json = json.dumps(dict)\n",
    "    f = open(\"embedding.json\",\"w\")\n",
    "    f.write(json)\n",
    "    f.close()\n",
    "\n",
    "def sparse_scipy_to_sparse_torch(S):\n",
    "    val = S.data\n",
    "    row = S.nonzero()[0]\n",
    "    col = S.nonzero()[1]\n",
    "    return torch.sparse_coo_tensor(indices = torch.tensor([row,col]), values = torch.tensor(val), size=[S.get_shape()[0],S.get_shape()[1]], dtype=torch.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lower it's rank\n",
    "\n",
    "def condition(V, target):\n",
    "    \n",
    "    #L, d, R = np.linalg.svd(V)\n",
    "    L, d, R = torch.svd(V, compute_uv=False)\n",
    "    \n",
    "    return (d[target-1]/d[target] < 10)\n",
    "\n",
    "def mix(V, steps, sigma):\n",
    "    for i in range(steps):\n",
    "\n",
    "        U1, d, U2 = torch.svd(V)\n",
    "        S = torch.diag(d)\n",
    "        \n",
    "        u_r = np.c_[U1[:, r].numpy()] #r-th left-eigenvetor\n",
    "        v_r = np.r_[[U2[:, r].numpy()]] #r-th right-eigenvector\n",
    "        PenGrad = torch.from_numpy(np.dot(u_r, v_r))\n",
    "        \n",
    "        first_term = torch.mm(V, torch.mm(V.T, V))#torch.mm(V, torch.mm(torch.mm(U2, S**2), U2.T))\n",
    "        C = 4*(first_term - torch.mm(Cooc_t, V))\n",
    "        \n",
    "        gradient = C/np.linalg.norm(C) + sigma*PenGrad/np.linalg.norm(PenGrad)\n",
    "        step = 1/np.linalg.norm(gradient)\n",
    "        V = V - step*(gradient) \n",
    "    return V\n",
    "\n",
    "#Entropy Penalised Word Embeddings\n",
    "def EPWE(V, steps, sigma, gamma, target_rank):\n",
    "    \n",
    "    while not condition(V, target_rank):\n",
    "        V = mix(V, steps, sigma)\n",
    "        sigma = sigma*gamma\n",
    "        \n",
    "    return V    \n",
    "    \n",
    "\n",
    "def cut(V, r):\n",
    "    L, d, R = np.linalg.svd(V)\n",
    "\n",
    "    dnew = copy.deepcopy(d)\n",
    "    dnew[range(r, len(d))] = 0\n",
    "    dmat = np.zeros((len(L), len(d)))\n",
    "    dmat[:len(d), :len(d)] = np.diag(dnew)\n",
    "\n",
    "\n",
    "    return np.dot(L, np.dot(dmat, R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions number:  19544 ; Words number:  905\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "\n",
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \",raw)\n",
    "    words = clean.split()\n",
    "    return words\n",
    "\n",
    "#choose question set\n",
    "#analogy = open('Analogy/analogy','r')\n",
    "analogy_sem = open('Analogy/google-sem.txt','r')\n",
    "analogy_syn = open('Analogy/google-syn.txt','r')\n",
    "\n",
    "#extract all words in analogy questions\n",
    "questions_sem = analogy_sem.read() \n",
    "questions_syn = analogy_syn.read()\n",
    "\n",
    "questions = questions_sem + questions_syn\n",
    "questions = questions.lower()\n",
    "questions_sem = questions_sem.lower()\n",
    "questions_syn = questions_syn.lower()\n",
    "\n",
    "list_of_questions = sent_tokenize(questions)\n",
    "list_of_questions_sem = sent_tokenize(questions_sem)\n",
    "list_of_questions_syn =sent_tokenize(questions_syn)\n",
    "\n",
    "number_of_questions = len(list_of_questions)\n",
    "\n",
    "list_of_words = list(set(sentence_to_wordlist(questions)))\n",
    "number_of_words = len(list_of_words)\n",
    "print(\"Questions number: \", number_of_questions, \"; Words number: \", number_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open embedding\n",
    "dimension = 50\n",
    "\n",
    "import json\n",
    "with open('Embeddings/glove-50.json', 'r') as data_file:\n",
    "    glove_emb = json.loads(data_file.read())\n",
    "    \n",
    "#target_rank\n",
    "r = 30     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#form a corresponding submatrix of corpus representations\n",
    "W = np.zeros((number_of_words, dimension))\n",
    "for word in list_of_words:\n",
    "       W[list_of_words.index(word), :] = glove_emb[word]\n",
    "        \n",
    "#reconstruct empirical cooc\n",
    "\n",
    "Cooc = np.zeros((number_of_words, number_of_words))\n",
    "for i in range(number_of_words):\n",
    "    for j in range(i):\n",
    "        Cooc[i,j] = np.dot(np.r_[W[i, :]], np.c_[W[j, :]])\n",
    "        Cooc[j, i] = Cooc[i,j]\n",
    "\n",
    "\n",
    "Cooc_sp = (scipy.sparse.coo_matrix(Cooc))\n",
    "Cooc_t = sparse_scipy_to_sparse_torch(Cooc_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best sigma:  1\n"
     ]
    }
   ],
   "source": [
    "#find the best sigma by random search\n",
    "V = torch.from_numpy(W)\n",
    "\n",
    "steps = 100\n",
    "sigma_best = 1\n",
    "V_best = mix(V, steps, sigma_best)\n",
    "bound = test_emb(list_of_questions_sem[:300], V_best)*len(list_of_questions_sem)/300\n",
    "\n",
    "for i in range(20):\n",
    "    sigma_curr = np.random.uniform(0.1, 10000)#sigma_best+1\n",
    "    V_curr = mix(V, steps, sigma_curr)\n",
    "    curr_bound = test_emb(list_of_questions_sem[:300], cut(V_curr, r))*len(list_of_questions_sem)/300\n",
    "    \n",
    "    if (curr_bound > bound):\n",
    "        sigma_best = sigma_curr\n",
    "        bound = curr_bound\n",
    "        V_best = V_curr\n",
    "                \n",
    "print(\"best sigma: \", sigma_best)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Semantic questions:\n",
      "EP emb:  0.23045435939418749\n",
      "PCA emb:  0.21561604584527222\n",
      "Original emb:  0.23239869013507983\n",
      "Results for Syntactic questions:\n",
      "EP emb:  0.35468686041751946\n",
      "PCA emb:  0.3286430618092509\n",
      "Original emb:  0.35934302087597214\n"
     ]
    }
   ],
   "source": [
    "print(\"Results for Semantic questions:\")\n",
    "\n",
    "print(\"EP emb: \", test_emb(list_of_questions_sem, V_best))\n",
    "print(\"PCA emb: \", test_emb(list_of_questions_sem, cut(W, r)))\n",
    "print(\"Original emb: \", test_emb(list_of_questions_sem, W))\n",
    "\n",
    "print(\"Results for Syntactic questions:\")\n",
    "\n",
    "print(\"EP emb: \", test_emb(list_of_questions_syn, V_best))\n",
    "print(\"PCA emb: \", test_emb(list_of_questions_syn, cut(W, r)))\n",
    "print(\"Original emb: \", test_emb(list_of_questions_syn, W))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare with a random projected embedding\n",
    "import math\n",
    "\n",
    "def RandProj(start_dim, target_dim):\n",
    "    \n",
    "    A = np.random.normal(0, 1, (start_dim, target_dim))\n",
    "    return A/math.sqrt(target_dim)\n",
    "\n",
    "def RandProjEmb(V, target_dim):\n",
    "    \n",
    "    return np.dot(V, RandProj(len(V[0, :]), target_dim))\n",
    "\n",
    "\n",
    "ProjEmb = RandProjEmb(W, 30)\n",
    "\n",
    "print(\"Results for Semantic questions:\")\n",
    "print(\"Random Projected Embedding performance: \", test_emb(list_of_questions_sem, ProjEmb))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
